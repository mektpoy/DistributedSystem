# 分布式文件系统作业
计55 许翰翔 2015011370

## 负载均衡

设计你的负载均衡算法，包括如何搜集ChunkServer的负载，如何指导客户端选择最合适负载的ChunkServer来存储新创建的Chunk？

考虑系统的工作负载：主要为大规模的流式读取和小规模的随机读取、大规模的数据追加式写操作。由于磁盘的速度通常比数据在机器之间传输的速度大得多，事实上的主要瓶颈在于网络的带宽，于是我们的目的在于充分利用各个ChunkSevrer的网络带宽。

于是搜集的信息就很显然了，需要统计一定时间内该ChunkServer的**网络利用情况**，对于网络利用率偏高的ChunkServer不应用来存储新创建的Chunk。

此外，一定时间内的网络利用情况并不能满足实际需求，因为会出现某个时间段，这个chunk是热点，而过了时效之后便无人问津，此时需要一些别的指标来均衡，比如ChunkServer的**Chunk总数**。

最后，由于新创建的chunk很可能在未来的一段时间内是热点数据，所以新的chunk最好不要都放在同一个ChunkServer，需要某种指标例如近10个新Chunk的平均创建时间去统计ChunkServer的**"新鲜程度"**。

综上：

- 当前选择的ChunkServer的负载较低
- 各个ChunkServer的chunk总数尽量平均
- 同一个ChunkServer不应连续被选择

## 元数据分布到多台节点

在分布式文件系统设计的时候，将元数据存放到一个节点上是一种解决方案。设计一种将元数据分布到多台节点上的方案，并分析其优缺点。

Google File System将元数据存放在唯一的master节点（并且是直接存在内存）上，这样做带来的一个可能的问题是，由于所有元数据都在一台机器上，Chunk的数量以及整个系统的承载能力都受限于 Master服务器所拥有的内存大小。这可能会成为整个系统的瓶颈。

考虑一个将元数据分布在4台机器上的方案，称这四台机器为MetaServer。仍然有一个Master节点负责调度，可以很方便的借助文件的哈希值设计一个到MetaServer的映射，快速找到MetaServer的编号。当客户端向Master请求租约时，Master需要先计算出实际储存元数据的MetaServer，并向之发送请求。MetaServer得到请求后找到对应的Chunk标识符及其副本，将信息传回Master，最终再由Master传回给客户端。

这样做的好处在于我们获得了更多的承载元数据的能力，但同时代价也是惨痛的，Master的网络带宽不仅要处理和客户端之间的通讯，还需要额外处理与MetaServer之间的通讯，并且这一步的网络延迟影响整个系统的运行速度。

事实上，MetaData需要存储在Master的信息并不多，大致在64字节左右，实际信息仍然存在了ChunkServer，Google File System将MetaData全部放在Master的内存中的做法是可以接受的。而且升级内存的开销要小得多，也不需要额外设计复杂的与MetaServer通讯和分配MetaServer的算法，并且由于内存的速度很快，Master的网络带宽可以充分的利用在于客户端之间的通讯，我认为是一个良好的解决方案。

## 文件一致性问题
对于Posix语义来说，一个一致性的要求就是如果写入数据成功，文件系统返回成功，那么读出的数据应该就是最后一次写入的数据。但是，在Google File System中并不保证这一点。仔细阅读论文，举出例子来说明这种情况如何发生？了能够保证Posix的语义，在分布式文件系统中可以通过什么手段达到？在Google File System中，出现这种情况，应用程序如何处理？

当写入数据成功时，读出数据不是最后一次写入数据的情况：

- 客户端缓存的Chunk位置数据在挂掉重启的ChunkServer中时，读出了过时内容
- 从影子Master读取到过时的MetaData信息

论文中提到了一些做法，例如对每个Chunk维护一个**版本号**，用以区别过时的版本，当发现版本过时后，从**快照**中获取一个较新的版本，并根据记录下来的**修改日志**维护Chunk，从而同步至最新的版本。对于影子Master可能产生的不一致的问题，由于影子Master主要是为了提高读取的效率，并且滞后的时间大约只是网络延迟(1s)的时间，当Master正常后，也不会出现读取过时MetaData信息的情况了。

对于应用程序的处理方式则是校验Master上提供的Chunk版本号和实际获得数据时的版本号，以及对影子Master获取Chunk后发现存在问题及时向Master请求正确的MetaData信息，或者重新向影子Master获取正确的信息。